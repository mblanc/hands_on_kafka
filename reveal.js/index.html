<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>reveal.js - The HTML Presentation Framework</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>Kafka</h1>
					<h3>par la face Nord</h3>
				</section>

				<section>
					<h2>Infrastructure</h2>
					<section>
						<p>Préparons nous</p>
					</section>
					<section>
						<p>Copier l'install de Kafka 0.9 sur votre machine depuis le site <a href="http://kafka.apache.org/downloads.html">Kafka Download</a></p>
					</section>
					<section>
						<p>Par la suite, les commandes exécutées supposent que vous êtes avec un terminal dans ce répertoire.</p>
					</section>
					<section>
						<p>Pensez à démarrer 5 à 6 sessions de terminal dans ce répertoire.</p>
					</section>
				</section>

				<section>
					<h2>Zookeeper</h2>
					<section>
						<p>Zookeeper sert à la coordination de Kafka.</p>
					</section>
					<section>
						<p>Il faut donc le démarrer en premier.</p>
						<p>Pour les besoins de l'atelier, un seul noeud suffit.</p>
					</section>
					<section>
						<h3>Démarrons 1 noeud Zookeeper</h3>
						<pre><code class="hljs" data-trim>./bin/zookeeper-server-start.sh config/zookeeper.properties</code></pre>
					</section>
					<section>
						<h3>Testons</h3>
						<pre><code class="hljs" data-trim>
telnet 127.0.0.1 2181

Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
						</code></pre>
					</section>					
				</section>

				<section>
					<h2>Kafka</h2>
					<section>
						<p>Ce sont ces noeuds qui vont gérer le stockage des messages et leur distribution.</p>
					</section>
					<section>
						<h3>Modification de la configuration du serveur</h3>
						<pre><code class="hljs" data-trim>						
//Changez la configuration par défaut avec 4 partitions
vi config/server.properties
...
num.partitions=4
...
						</code></pre>
					</section>
					<section>
						<h3>Création de deux fichiers de configuration</h3>
						<pre><code class="hljs" data-trim>
//Copiez le fichier dans deux fichiers séparés, un par broker
cp config/server.properties config/server-1.properties
cp config/server.properties config/server-2.properties
						</code></pre>
					</section>
					<section>
						<h3>Configuration du noeud 1</h3>
						<p>server-1.properties</p>
						<pre><code class="hljs" data-trim>
//Éditez le fichier config/server-1.properties pour changer son id, port et répertoire de travail
vi config/server-1.properties
...
broker.id=1
...
port=9092
...
log.dirs=/tmp/kafka-logs-1
...
zookeeper.connect=localhost:2181
...
                            zookeeper déjà à la bonne valeur
						</code></pre>
					</section>
					<section>
						<h3>Configuration du noeud 2</h3>
						<p>server-2.properties</p>
						<pre><code class="hljs" data-trim>
//Éditez le fichier config/server-2.properties pour changer son id, port et répertoire de travail
vi config/server-2.properties
...
broker.id=2
...
port=9093
...
log.dirs=/tmp/kafka-logs-2
...
zookeeper.connect=localhost:2181
...
                            zookeeper déjà à la bonne valeur
                            listeners=PLAINTEXT://:9093 à modifier aussi
						</code></pre>
					</section>
					<section>
						<h3>Démarrons les deux noeuds</h3>
						<pre><code class="hljs" data-trim>
//Broker 1
//Lancez le noeud
./bin/kafka-server-start.sh config/server-1.properties
							
...

//Broker 2
//Lancez le noeud
./bin/kafka-server-start.sh config/server-2.properties
						</code></pre>
					</section>
					<section>
						<h3>Testons</h3>
						<pre><code class="hljs" data-trim>
//Connectez vous à Zookeper
./bin/zookeeper-shell.sh 127.0.0.1:2181

//Vérifiez les ids des brokers
ls /brokers/ids

//Vérifiez le port du broker1
get /brokers/ids/1

//Vérifiez le port du broker2
get /brokers/ids/2
						</code></pre>
					</section>
				</section>

				<section>
					<h2>Kafka CLI</h2>
					<section>
						<p>Nous allons créer un topic pour cet atelier. Il sera répliqué une seule fois avec 4 partitions.</p>
					</section>
					<section>
						<h3>Création du topic 'handsonkafka'</h3>
						<pre><code class="hljs" data-trim>
./bin/kafka-topics.sh --create --topic handsonkafka --partition 4 --replication-factor 1 --zookeeper 127.0.0.1:2181
						</code></pre>
					</section>
					<section>
						<p>Vous remarquerez que nous ne discutons pas directement avec Kafka mais seulement avec Zookeeper.</p>
						<p>Kafka est par nature distribué, chaque broker surveille Zookeeper. L'ensemble se coordonne ensuite pour avoir un état cohérent.</p>
					</section>
					<section>
						<h3>Vérification</h3>
						<pre><code class="hljs" data-trim>
./bin/kafka-topics.sh --describe  --topic handsonkafka --zookeeper 127.0.0.1:2181
Topic:handsonkafka	PartitionCount:4	ReplicationFactor:1	Configs:
Topic: handsonkafka	Partition: 0	Leader: 2	Replicas: 2	Isr: 2
Topic: handsonkafka	Partition: 1	Leader: 1	Replicas: 1	Isr: 1
Topic: handsonkafka	Partition: 2	Leader: 2	Replicas: 2	Isr: 2
Topic: handsonkafka	Partition: 3	Leader: 1	Replicas: 1	Isr: 1
						</code></pre>
						<p>Dans ce cas, les partitions 0 et 2 sont gérées par le broker 2, 1 et 3 par le broker 1.</p>
					</section>
					<section>
						<h3>Lancement d'un consommateur</h3>
						<pre><code class="hljs" data-trim>
./bin/kafka-console-consumer.sh --topic handsonkafka --zookeeper 127.0.0.1:2181
						</code></pre>
						<p>Rien ne se passe pour l'instant car il n'y a pas de production de données. Par contre, vous aurez remarqué que des logs sont apparues dans les brokers.</p>
					</section>
					<section>
                        <pre><code class="hljs" data-trim>
./bin/kafka-topics.sh --describe  --topic handsonkafka --zookeeper 127.0.0.1:2181
                            
Topic:handsonkafka	PartitionCount:4	ReplicationFactor:1	Configs:
Topic: handsonkafka	Partition: 0	Leader: 2	Replicas: 2	Isr: 2
Topic: handsonkafka	Partition: 1	Leader: 1	Replicas: 1	Isr: 1
Topic: handsonkafka	Partition: 2	Leader: 2	Replicas: 2	Isr: 2
Topic: handsonkafka	Partition: 3	Leader: 1	Replicas: 1	Isr: 1
                        </code></pre>
					</section>
                    <section>
                        <h3>Lancement d'un producteur</h3>
						<pre><code class="hljs" data-trim>
./bin/kafka-console-producer.sh --topic handsonkafka --broker-list 127.0.0.1:9092,127.0.0.1:9093
                        </code></pre>
                        <p>Vous remarquerez cette fois que nous avons donné en paramètre la liste des brokers et pas de zookeeper.</p>
                    </section>
                    <section>
                        <p>Saississez des messages séparés par ENTER. Ils doivent apparaître dans la session du consommateur.</p>
                    </section>
                    <section>
                        <h3>Stockage des données</h3>
                        <pre><code class="hljs" data-trim>
ls -l /tmp/kafka-logs-*
                            
/tmp/kafka-logs-1:
total 16
.
..
handsonkafka-1
handsonkafka-3

/tmp/kafka-logs-2:
total 16
.
..
handsonkafka-0
handsonkafka-2
                        </code></pre>
                    </section>
                    <section>
                        <h3>Stockage des données</h3>
                        <pre><code class="hljs" data-trim>
ls -l /tmp/kafka-logs-*/handsonkafka-*/*
                            
/tmp/kafka-logs-1/handsonkafka-1/00000000000000000000.index
/tmp/kafka-logs-1/handsonkafka-1/00000000000000000000.log
/tmp/kafka-logs-1/handsonkafka-3/00000000000000000000.index
/tmp/kafka-logs-1/handsonkafka-3/00000000000000000000.log
/tmp/kafka-logs-2/handsonkafka-0/00000000000000000000.index
/tmp/kafka-logs-2/handsonkafka-0/00000000000000000000.log
/tmp/kafka-logs-2/handsonkafka-2/00000000000000000000.index
/tmp/kafka-logs-2/handsonkafka-2/00000000000000000000.log
                        </code></pre>
                        <p>Dans chaque partition, un fichier a été créé pour indexer les données et les stocker.</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
cat /tmp/kafka-logs-*/handsonkafka-*/*.log
p_N����Bonjour handsonkafkap_N����Bonjour handsonkafkap_N����Bonjour handsonkafka�������phae�[^<����ahize%
                        </code></pre>
                        <p>Enfin, à l'intérieur des fichiers .log (segments) se trouvent les messages.</p>
                    </section>
				</section>
                
                <section>
                    <h2>STEP_1</h2>
                    <h3>Codons un producteur</h3>
                    <section></section>
                    <section>
                        <p>Nous allons maintenant créer un producteur de données avec du code et l'API 0.9.0.X de Kafka.</p>
                    </section>
                    <section>
                        <p>Écrire dans Kafka consiste à :
                            <ol>
                            <li>créer un client Zookeeper</li>
                            <li>récupérer les adresses des noeuds Kafka (ou brokers)</li>
                            <li>instancier un KafkaProducer</li>
                            <li>écrire dans le topic</li>
                            </ol>
                        </p>
                    </section>
                    <section>
                        <p>Pour se faire, vous allez devoir suivre les 4 TODO de la classes <i>fr.xebia.xebicon.kafka.Producer</i> en suivant les étapes décrites si dessous.</p>
                        <p>Cette classe est une classe exécutable que vous pouvez lancer depuis SBT, Eclipse ou IntelliJ.</p>
                    </section>
                </section>
                
                
                <section>
                    <h2>STEP_1_1</h2>
                    <h3>Création d'un client Zookeeper</h3>
                    <section></section>
                    <section>
                        <pre><code class="hljs" data-trim>
def connectToZookeeper(): ZkUtils = {
    ZkUtils("127.0.0.1:2181", 10000, 5000, false)
}
                        </code></pre>
                        <p>Créer un client zookeeper est simple.</p>
                        </section>
                    <section>
                        <p>Dans un environnement de production, il est nécessaire de préciser l'adresse de l'ensemble des noeuds Zookeeper.</p>
                        <p>Zookeeper préfère ne pas donner de valeur plutôt qu'une mauvaise. Ainsi un client Zookeeper doit connaître l'ensemble des noeuds pour avoir toujours la bonne version de la donnée.</p>
                    </section>
                    <section>
                        <p>Cette portion n'est pas vraiment obligatoire. Dans un environnement où les noeuds Kafka sont stables avec IP,PORT connus, il n'est pas nécessaire d'aller les chercher dans Zookeeper.</p>
                    </section>
                    <section>
                        <p>Cette partie de "service discovery" est néanmoins un classique dans les systèmes distribués et reste nécessaire pour le SimpleConsumer.</p>
                        <p>Vous n'aurez donc pas perdu votre temps :)</p>
                    </section>
                </section>
              
              
                <section>
                    <h2>STEP_1_2</h2>
                    <h3>Récupération des brokers</h3>
                    <section></section>
                    <section>
                        <p>Kafka fournit cette API grâce à la classe <i>kafka.utils.ZkUtils</i>. Vous trouverez alors la liste de tous les brokers du système.</p>
                        <p>Il vous faudra ensuite concatener avec ',' la liste des attributs <i>connectionString</i> de chaque broker.</p>
                    </section>
                    <section>
                        <h4>Petites aides en Scala ?</h4>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
//extrait de chaque broker son url de connection
def extractConnectionStringFrom(brokers: Seq[Broker]): Seq[String]

//concatène les url en les séparant par une virgule
def join(brokers: Seq[String]): String
                        </code></pre>
                    </section>
                    <section>
                        <h4>Solution</h4>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
import kafka.cluster.Broker

def extractConnectionStringFrom(brokers: Seq[Broker]): Seq[String] =
    brokers.map(_.getBrokerEndPoint(SecurityProtocol.PLAINTEXT).connectionString)

def join(brokers: Seq[String]): String = brokers.mkString(",")

def brokersFromZk: Seq[Broker] = zkUtils.getAllBrokersInCluster()

join(
    extractConnectionStringFrom(
        brokersFromZk
    )
)
                        </code></pre>
                    </section>
                </section>
                
                
                <section>
                    <h2>STEP_1_3</h2>
                    <h3>Instanciation d'un KafkaProducer</h3>
                    <section></section>
                    <section>
                        <p>La classe à utiliser est <i>org.apache.kafka.clients.producer.KafkaProducer[K,V]</i>.</p>
                    </section>
                    <section>
                        <p>Un producer a besoin d'un Map de configuration</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def props = Map(
    "value.serializer" -> "org.apache.kafka.common.serialization.StringSerializer",
    "key.serializer" -> "org.apache.kafka.common.serialization.StringSerializer",
    "partitioner.class" -> "kafka.producer.DefaultPartitioner", => attention, n'a plus de constructeur sans argument
    "max.request.size" -> "10000",
    "bootstrap.servers" -> connectionString,
    "acks" -> "all",
    "retries" -> "3",
    "retry.backoff.ms" -> "500"
)
                        </code></pre>
                    </section>
                    <section>
                        <p><i>org.apache.kafka.common.serialization.StringSerializer</i></p>
                        <p>se contente de passer tout le contenu du message de type String en clair sur le disque.</i></p>
                    </section>
                    <section>                        
                        <p><i>kafka.producer.DefaultPartitioner</i></p>
                        <p>permet de choisir la stratégie de routage des messages d'un topic vers une partition. Ici c'est du RoundRobin.</i></p>
                    </section>
                    <section>                        
                        <p><i>max.request.size</i></p>
                        <p>est un paramètre important. Il s'agit de la taille maximum autorisée. Au dessus, le producteur lancer une exception. Nous verrons son importance côté consommateur plus tard dans l'exercice.</i></p>
                    </section>
                    <section>
                        <p><i>import ...wrapAsJava._</i></p>
                        <p>peut sembler magique. Cela permet seulement de convertir la Map Scala en Map Java.</p>
                    </section>
                    <section>
                        <h4>Solution</h4>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def createKafkaProducer(connectionString:String): KafkaProducer[Any, Any] = {
    import scala.collection.JavaConversions._

    def props = Map(
        "value.serializer" -> "org.apache.kafka.common.serialization.StringSerializer",
        "key.serializer" -> "org.apache.kafka.common.serialization.StringSerializer",
        "partitioner.class" -> "kafka.producer.DefaultPartitioner",
        "max.request.size" -> "10000", "acks" -> "all",
        "bootstrap.servers" -> connectionString,
        "retries" -> "3", "retry.backoff.ms" -> "500"
    )

    new KafkaProducer[Any, Any](props)
}
                        </code></pre>
                    </section>
                </section>
                
                
                <section>
                    <h2>STEP_1_4</h2>
                    <h3>Écriture dans le topic</h3>
                    <section></section>
                    <section>
                        <p><i>org.apache.kafka.clients.producer.ProducerRecord[K, V]</i></p>
                        <p>Explorez l'API de <i>KafkaProducer</i> et <i>ProducerRecord</i> et envoyez un message à l'aide d'un <i>ProducerRecord[Any,Any]</i> sur le topic <i>handsonkafka</i>.</p>
                    </section>
                    <section>
                        <p>Vous trouverez une méthode <i>produceData</i> qui fait une boucle et produit un message avec le timestamp + la charge moyenne de votre machine.</p>
                    </section>
                    <section>
                        <p>L'API est asynchrone.</p>
                        <p>Vous allez devoir bloquer sur ce Future de Java.</p>
                    </section>
                    <section>
                        <h4>Petites aides en Scala ?</h4>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
//bloque sur le future Java
def blockOn[T](javaFuture:Future[T]):T
                        </code></pre>
                    </section>
                    <section>
                        <h4>Solution</h4>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
Stream.continually(Instant.now()).foreach { instant =>
    val averageSystemLoad = ManagementFactory.getOperatingSystemMXBean.getSystemLoadAverage

    def data = s"$instant: avg_load: $averageSystemLoad"

    val messageSending: Future[RecordMetadata] = producer.send(new ProducerRecord[Any, Any]("handsonkafka", data))

    blockOn(messageSending)

    Thread.sleep(1000)
}</code></pre>
                    </section>
                    <section>
                        <p><i>Thread.sleep(1000)</i> est juste là pour ne pas saturer le système. Essayer de changer sa valeur pour voir la différence sur la charge sur votre système.</p>
                    </section>
                    <section>
                        <p>Vérifier dans vos préférences projet et IntelliJ/Eclipse que le projet et le compilateur Scala sont en JDK 1.8.</p>
                    </section>
                    <section>
                        <h3>kafka-console-consumer.sh</h3>
                        <pre><code class="hljs" data-trim>
2015-10-20T16:11:17.412Z: avg_load: 2.81005859375
2015-10-20T16:11:18.934Z: avg_load: 2.81005859375
2015-10-20T16:11:19.939Z: avg_load: 2.81005859375
2015-10-20T16:11:20.946Z: avg_load: 2.6650390625
2015-10-20T16:11:21.951Z: avg_load: 2.6650390625
                        </code></pre>
                    </section>
                </section>
                
                
                
                
                
                
                
                
                
                <section>
                    <h2>STEP_2</h2>
                    <h3>HIGH LEVEL CONSUMER</h3>
                    <section></section>
                    <section>
                        <p>En suivant la même démarche, nous allons coder un consommateur haut-niveau.</p>
                        <p>Il fonctionne quasiment tout seul et offre un service de fail-over.</p>
                    </section>
                    <section>
                        <p>Cette fois, nous allons travailler sur la classe <i>fr.xebia.xebicon.kafka.ConsumerHighLevel</i>.</p>
                    </section>
                    <section>
                        <p>Lire avec ce consommateur depuis Kafka consiste à :<ol>
                            <li>configuer un connector</li>
                            <li>créer un stream pour des couples topic/partitions</li>
                            <li>itérer sur les streams de manière concurrente</li>
                        </ol>
                        </p>
                    </section>
                </section>
                
                
                <section>
                    <h2>STEP_2_1</h2>
                    <h3>Configuration d'un connector</h3>
                    <section></section>
                    <section>
                        <p>L'API de haut niveau permet de créer très rapidement un consommateur. Pour ce faire, il lui faut:<ul>
                         <li>l'adresse de Zookeeper</li>
                         <li>l'identifiant du groupe de consommation</li>
                         <li>les paramètres d'autocommit</li>
                        </ul></p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
val props = new Properties()

props.put("zookeeper.connect", zookeeper)
props.put("group.id", groupId)
props.put("zookeeper.session.timeout.ms", "400")
props.put("zookeeper.sync.time.ms", "200")
props.put("auto.commit.interval.ms", "1000")
                        </code></pre>
                    </section>
                    <section>
                        <h4>Solution</h4>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def groupId = "test"
def zookeeper = "127.0.0.1:2181"

val props = new Properties()

props.put("zookeeper.connect", zookeeper)
props.put("group.id", groupId)
props.put("zookeeper.session.timeout.ms", "400")
props.put("zookeeper.sync.time.ms", "200")
props.put("auto.commit.interval.ms", "1000")

Consumer.create(new ConsumerConfig(props))
                        </code></pre>
                    </section>
                    <section>
                        <p>Ce consommateur est en auto-commit à intervalle régulier. Il peut convenir à la majorité des cas mais pas forcément tous.</p>
                        <p>Cela veut dire qu'il est possible d'avoir à traiter plusieurs fois le même message.</p>
                    </section>
                    <section>
                        <p>En fonction de votre SLA, faites le choix qui convient.</p>
                    </section>
                </section>
                
                
                <section>
                    <h2>STEP_2_2</h2>
                    <h3>Création du stream</h3>
                    <section></section>
                    <section>
                        <p>À partir du connector, il est possible de demander la création d'un stream sur un couple topic/partition.</p>
                        <p>C'est l'API qui se charge ensuite de créer la connection avec le broker, initier le stream de données, puis les phases d'auto-commit.</p>
                    </section>
                    <section>
                        <p>Créer donc un stream sur notre topic *xebicon* pour ses *4* partitions.</p>
                    </section>
                    <section>
                        <h4>Solution</h4>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def topic = "handsonfkafka"
def numberOfPartitions = 4

consumer.createMessageStreams(Map(topic -> numberOfPartitions))(topic)
                        </code></pre>
                    </section>
                </section>


                <section>
                    <h2>STEP_2_3</h2>
                    <h3>Itération sur les messages</h3>
                    <section></section>
                    <section>
                        <p>Un stream de message est principalement composé d'un itérateur de Array[Byte].</p>
                        <p>Les partitions représentent le niveau de "parallélisation" d'un système Kafka.</p>
                        <p>Nous allons donc ici créer un pool de thread par défaut et ainsi lire chaque partition dans un thread.</p>
                    </section>
                    <section>
                        <p>Bouclez sur l'itérateur de message et appeler pour chaque message la méthode <i>display</i>.</p>
                    </section>
                    <section>
                        <p>Petites aides en Scala ?</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
//cette ligne importe dans le contexte un pool de threads par défaut
import concurrent.ExecutionContext.Implicits.global

Future {
//la portion de code ici est exécutée en asynchrone
}
                        </code></pre>
                    </section>
                    <section>
                        <p>Solution</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def consumeStreamFrom(partitionStream: KafkaStream[Array[Byte], Array[Byte]]): Unit = {

    def display(message: MessageAndMetadata[Array[Byte], Array[Byte]]): Unit = {
        def payload: String = new String(message.message(), "UTF-8")
        def partition: Int = message.partition
        def offset: Long = message.offset

        println(s"partition: $partition, offset: $offset: $payload")
    }

    import concurrent.ExecutionContext.Implicits.global
    Future {
        partitionStream.iterator().foreach(message => display(message))
    }
}
                        </code></pre>
                    </section>
                    <section>
                        <p>À ce stade, vous pouvez lancer la classe <i>fr.xebia.xebicon.kafka.ConsumerHighLevel</i> et voir les messages envoyés par le producer.</p>
                        <p>Vous pouvez donc fermer le consommateur en CLI.</p>
                    </section>
                </section>

                <section>
                    <h2>HIGH LEVEL CONSUMER</h2>
                    <section></section>
                    <section>
                        <p>Kafka agit comme un broker de messages.</p>
                        <p>Arrêtez le consommateur quelques secondes, relancer le, vous verrez alors que le consommateur reprend là où il s'était arrêté.</p>
                        <p>Démarrez une seconde instance de Consumer, rien ne lui parviendra car la première JVM lit les messages du topic. Arrêter la première instance et la second prend le relai.</p>
                    </section>
                    <section>
                        <p>Stoppez tous les consommateurs et changer le nombre de partitions lues à 2 dans la configuration du connecteur. Lancez deux instances du consommateur, les deux prennent 50% des messages.</p>
                        <p>Stoppez en 1 et l'autre reprendra ses partitions.</p>
                    </section>
                    <section>
                        <p>Le <i>High Level consumer</i> convient à la majorité des cas. Si l'autocommit n'est pas un problème, c'est cette API qu'il faut utiliser.</p>
                    </section>
                    <section>
                        <p>Pour cet atelier, nous allons aller plus loin dans la gestion de stream pour le parcourir à l'envers. </p>
                        <p>On transforme ainsi un consommateur type FIFO en LIFO!</p>
                        <p>Pour cela, il faut descendre d'un cran et utiliser le <i>Simple Consumer</i>.</p>
                        <p>Let's go!</p>
                    </section>
                </section>


                <section>
                    <h2>STEP_3</h2>
                    <h3>SIMPLE CONSUMER</h3>
                    <section></section>
                    <section>
                        <p>Utiliser le <i>SimpleConsumer</i>, c'est comprendre comment Kafka fonctionne.</p>
                        <p>Cela démontre sa nature distribuée et son mode de fonctionnement.</p>
                    </section>
                    <section>
                        <p>Les étapes sont beaucoup plus nombreuses que précédemment.</p>
                        <p>Nous n'allons simplifier certaines étapes.</p>
                    </section>
                    <section>
                        <p>Le but de cet exercice est de comprendre comment est structuré la consommation de données dans Kafka.</p>
                        <p>Nous ne ferons pas la gestion du commit qui demanderait avec la version 0.9.0.X trop de complexité de code à réaliser en si peu de temps.</p>
                        <p>Nous allons plutôt parcourir parcourir tous les messages des partitions d'un topic, mais du plus récent au plus ancien!</p>
                        </p>
                    </section>
                    <section>
                        <p>Nous allons travailler sur la classe <i>fr.xebia.xebicon.kafka.LowLevelConsumer</i>.</p>
                    </section>
                    <section>
                        <p>Lire avec ce consommateur depuis Kafka consiste à :<ul>
                        <li>créer un client Zookeeper</li>
                        <li>récupérer tous les brokers</li>
                        <li>récupérer depuis zookeeper les métadonnées du topic concerné</li>
                    </ul>
                    </section>
                    <section>
                        <p><ul>
                        <li>trouver le broker leader de chaque partition</li>
                        <li>créer une connexion à ce broker</li>
                        <li>chercher l'offset du plus ancien et plus récent message consommé</li>
                        <li>récupérer les messages dans l'ordre décroissant</li>
                    </ul>
                        </p>
                    </section>
                    <section>
                        <p>On se retrousse les manches, on prend son piolet et on y va!</p>
                    </section>
                </section>

                
                <section>
                    <h2>STEP_3_1</h2>
                    <h3>Création d'un client Zookeeper</h3>
                    <section></section>
                    <section>
                        <p>Rien de bien difficile ici, nous avons déjà effectué cette étape en STEP_1_1.</p>
                    </section>
                    <section>
                        <p>Solution</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def connectToZookeeper(): ZkUtils = {
    ZkUtils("127.0.0.1:2181", 10000, 5000, false)
}
                        </code></pre>
                    </section>
                </section>


                <section>
                    <h2>STEP_3_2</h2>
                    <h3>Récupération de la configuration du topic</h3>
                    <section></section>
                    <section>
                        <p>Pour pouvoir consommer des messages, il faut récupérer dans les metadata du topic le nombre de partitions configurés.</p>
                    </section>
                    <section>
                        <p>À vous de chercher dans <i>kafka.admin.AdminUtils</i> la bonne méthode.</p>
                    </section>
                    <section>
                        <p>Solution</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def fetchTopicMetadata(topic: String, zkUtils: ZkUtils): TopicMetadata = {
    import kafka.admin.AdminUtils

    AdminUtils.fetchTopicMetadataFromZk(topic, zkUtils)
}
                        </code></pre>
                    </section>
                </section>


                <section>
                    <h2>STEP_3_3</h2>
                    <h3>Recherche du leader de chaque partition</h3>
                    <section></section>
                    <section>
                        <p>Il existe à chaque instant au plus 1 noeud Kafka leader pour une partition d'un topic donné.</p>
                    </section>
                    <section>
                        <p>À vous de chercher dans <i>kafka.admin.AdminUtils</i> la bonne méthode.</p>
                    </section>
                    <section>
                        <p>Petites aides en Scala ?</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
//permet de transformer l'objet TopicMetadata en Map avec clé le numéro de la partition, en valeur les métadatas de la partition.
def asMap(topicMetadata: TopicMetadata): Map[Int, PartitionMetadata] = {
    topicMetadata.partitionsMetadata.groupBy(_.partitionId).toMap.mapValues(_.head)
}

//Sur un objet de type Map, mapValues permet d'extraire une donnée des valeurs d'une MAP.
partitionsMetadata.mapValues(metadata => ???)
                        </code></pre>
                    </section>
                    <section>
                        <p>Solution</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def findPartitionLeader(topicMetadata: TopicMetadata): Map[Int, Option[BrokerEndPoint]] = {
    import kafka.api.PartitionMetadata

    def asMap(topicMetadata: TopicMetadata): Map[Int, PartitionMetadata] = {
        topicMetadata.partitionsMetadata.groupBy(_.partitionId).toMap.mapValues(_.head)
    }
    val partitionsMetadata: Map[Int, PartitionMetadata] = asMap(topicMetadata)

    partitionsMetadata.mapValues(metadata => metadata.leader)
}
                        </code></pre>
                    </section>
                    <section>
                        <p>La méthode <i>findPartitionLeader</i> retourne <i>Option[BrokerEndPoint]</i> car il se peut qu'il n'y ait pas de leader à cet instant.</p>
                    </section>
                </section>

                
                <section>
                    <h2>STEP_3_4</h2>
                    <h3>Connexion aux brokers</h3>
                    <section></section>
                    <section>
                        <p>À ce stade, nous avons la liste des leaders de chaque partition.</p>
                        <p>Dans le cas où le leader existe, créer une connexion à l'aide de la classe <i>import kafka.consumer.SimpleConsumer</i>.</p>
                    </section>
                    <section>
                        <p>Solution</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def connectTo(leader: Broker): SimpleConsumer = {
    new SimpleConsumer(leader.host, leader.port, 10000, 64000, "xebicon-printer")
}
                        </code></pre>
                    </section>
                </section>


                <section>
                    <h2>STEP_3_5</h2>
                    <h3>Trouver l'offset de démarrage de consommation</h3>
                    <section></section>
                    <section>
                        <p>Une partition est un journal en ajout seul. Chaque message possède un numéro unique au sein d'une même partition.</p>
                        <p>Cet identifiant, issu d'un compteur monotonique (strictement croissant), est nommé <i>offset</i>.</p>
                    </section>
                    <section>
                        <p>Pour chaque requête de données à Kafka, il faut lui préciser le nombre de messages que l'on veut recevoir, et depuis quel offset.</p>
                    </section>
                    <section>
                        <p>Trouvez sur la classe <i>SimpleConsumer</i> une méthode permettant de trouver le premier et dernier offset d'une partition.</p>
                    </section>
                    <section>
                        <p>Solution</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def findEarliestOffset(partition: Int, consumer: SimpleConsumer): Long = {
    def earliestOffsetRequest = OffsetRequest.EarliestTime
    def consumerId = Request.OrdinaryConsumerId

    consumer.earliestOrLatestOffset(new TopicAndPartition("xebicon", partition), earliestOffsetRequest, consumerId)
}

def findLatestOffset(partition: Int, consumer: SimpleConsumer): Long = {
    def latestOffsetRequest = OffsetRequest.LatestTime
    def consumerId = Request.OrdinaryConsumerId

    consumer.earliestOrLatestOffset(new TopicAndPartition("xebicon", partition), latestOffsetRequest, consumerId)
}
                        </code></pre>
                    </section>
                </section>

                
                <section>
                    <h2>STEP_3_6</h2>
                    <h3>Faire une requête de données</h3>
                    <section></section>
                    <section>
                        <p>Maintenant que nous avons la connexion au leader et l'offset à demander, il n'y a plus qu'à récupérer la donnée.</p>
                        </section>
                    <section>
                        <p>Dans Kafka, on ne demande pas N messages. On demande une taille en bytes.</p>
                        <p>Dans la réponse, nous aurons ensuite un itérateur permettant de parcourir chaque message reçu. Il est donc <strong>important</strong> de connaître la taille des messages que l'on manipule.</p>
                    </section>
                    <section>
                        <p>Cela semble bizarre au début mais cela se révèle être un atout majeur en terme de performance.</p>
                        <p>En effet, toutes les I/O se mesurent en bytes: le réseau, les buffers, le disque... en ne manipulant que des tailles en bytes, il est ainsi possible d'être plus précis pour le tuning de performance.</p>
                    </section>
                    <section>
                        <p>À vous de: <ol>
                        <li>créer une FetchRequest grâce à <i>kafka.api.FetchRequestBuilder</i>.</li>
                        <li>l'exécuter avec le <i>SimpleConsumer</i>.</li>
                    </ol></p>
                    </section>
                    <section>
                        <p>Petites aides en Scala ?</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
//permet de faire une boucle depuis latestOffset à earliestOffset par étape de -1
//et stocke la variable d'itération dans offset
for (offset <- latestOffset.to(earliestOffset, step = -1))
yield {

    ...
                            
}
                        </code></pre>
                    </section>
                    <section>
                        <p>Solution</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def requestData(partition: Int, consumer: SimpleConsumer, offset: Long): FetchResponse = {
    def messageMaxSize = 1000
    def numberOfMessage = 1
    def clientId = "xebicon-printer"
    def topic = "xebicon"

    val request = new FetchRequestBuilder()
        .clientId(clientId)
        .addFetch(topic, partition, offset, numberOfMessage * messageMaxSize)
        .build()

    consumer.fetch(request)
}
                        </code></pre>
                    </section>
                    <section>
                        <p>NB: il est possible que Kafka vous envoie des messages un peu avant l'offset qui est demandé (pour des raisons d'optimisation).</p>
                        <p>Dans la vraie vie, pensez à filtrer sur les offsets des messages reçus.</p>
                    </section>
                </section>


                <section>
                    <h2>STEP_3_7</h2>
                    <h3>Afficher le message</h3>
                    <section></section>
                    <section>
                        <p>Le travail est presque terminé.</p>
                        <p>Sur l'objet de type <i>FetchResponse</i> il est possible de récupérer les messages grâce à la méthode <i>messageSet</i>.</p>
                        <p>Celui-ci fournit un itérateur sur les messages reçu aux travers de ByteBuffers.</p>
                    </section>
                    <section>
                        <p>NB: Le stream de données est découpé en message côté client.</p>
                    </section>
                    <section>
                        <p>Itérer maintenant sur le messageSet et récupérer le premier message pour l'afficher.</p>
                    </section>
                    <section>
                        <p>Petites aides en Scala ?</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
//permet de prendre le premier élément de l'itérateur, d'extraire le message et d'effectuer un traitement dessus
... .iterator.take(1).foreach {

    case MessageAndOffset(message, readOffset) =>

}

//permet d'extraite le payload d'un message
def readBytes(message: Message): String = {
    val content = Array.ofDim[Byte](message.payloadSize)
    message.payload.get(content, 0, message.payloadSize)
    new String(content, "UTF-8")
}
                        </code></pre>
                    </section>
                    <section>
                        <p>Solution</p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def consume(partition: Int, fetchReply: FetchResponse) {
    def readBytes(message: Message): String = {
        val content = Array.ofDim[Byte](message.payloadSize)
        message.payload.get(content, 0, message.payloadSize)
        new String(content, "UTF-8")
    }

    fetchReply.messageSet("xebicon", partition).iterator.take(1).foreach {
        case MessageAndOffset(message, readOffset) =>
            def offset: Long = readOffset
            val payload: String = readBytes(message)
            println(s"partition: $partition, offset: $offset. $payload")
    }
}
                        </code></pre>
                    </section>
                </section>

                <section>
                    <h2>STEP_4</h2>
                    <h3>NEW CONSUMER</h3>
                    <section></section>
                    <section>
                        <p>Nous allons maintenant utiliser le nouveau Consumer Kafka qui simplifie et unifie les concepts des deux Consumers précédents.</p>
                    </section>
                    <section>
                        <p>Nous allons travailler sur la classe <i>fr.xebia.xebicon.kafka.NewConsumer</i>.</p>
                    </section>
                    <section>
                        <p>Lire avec ce consommateur depuis Kafka consiste à :
                        <ol>
                            <li>configuer un KafkaConsumer</li>
                            <li>s'inscrire à une topic/partitions</li>
                            <li>faire un poll sur les messages de cette topic</li>
                        </ol>
                        </p>
                    </section>
                    <section>
                        <p>Lire la <a href="http://kafka.apache.org/090/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html">Javadoc!</a></p>
                        Attention : accès internet limité lors du handson
                    </section>
                </section>

                <section>
                    <h2>STEP_4_1</h2>
                    <h3>Configuration d'un KafkaConsumer</h3>
                    <section></section>
                    <section>
                        <p>Pour ce faire, il lui faut:<ul>
                        <li>l'adresse de un ou plusieurs brokers Kafka (et plus celle de Zookeeper)</li>
                        <li>un identifiant de group de consommation</li>
                        <li>les paramètres d'autocommit</li>
                    </ul></p>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
                            val props: Properties = new Properties
                            props.put("bootstrap.servers", "localhost:9092")
                            props.put("group.id", "test2")
                            props.put("enable.auto.commit", "false")
                            props.put("auto.commit.interval.ms", "500")
                            props.put("session.timeout.ms", "30000")
                            props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
                            props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")

                            ?? new KafkaConsumer(props)
                        </code></pre>
                    </section>
                    <section>
                        <h4>Solution</h4>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
                            val props: Properties = new Properties
                            props.put("bootstrap.servers", "localhost:9092")
                            props.put("group.id", "test2")
                            props.put("enable.auto.commit", "false")
                            props.put("auto.commit.interval.ms", "500")
                            props.put("session.timeout.ms", "30000")
                            props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
                            props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
                            new KafkaConsumer[String, String](props)
                        </code></pre>
                    </section>
                    <section>
                        <p>Ici on reproduit le comportement d'un consommateur de haut niveau en auto-commit</p>
                    </section>
                </section>

                <section>
                    <h2>STEP_4_2</h2>
                    <h3>Consommer les messages</h3>
                    <section></section>
                    <section>
                        <p>Inscrivez le KafkaConsumer au topic à l'aide de subscribe
                            <ul>
                                <li>Consommez les records à l'aide de poll</li>
                            </ul>
                        </p>
                    </section>
                    <section>
                        <h4>Solution</h4>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def consumeStreamFrom(consumer: KafkaConsumer[String, String]): Unit = {
    import scala.collection.JavaConversions._
    consumer.subscribe(Collections.singletonList(TOPIC))
    while (true) {
        val records: ConsumerRecords[String, String] = consumer.poll(100)
        for (record <- records) display(record)
    }
}
                        </code></pre>
                    </section>
                </section>

                <section>
                    <h2>Step 5</h2>
                    <h3>Spark Streaming Client</h3>
                    <section></section>
                    <section>
                        <p>Nous allons maintenant consommer les messages en streaming presque temps réel.</p>
                    </section>
                    <section>
                        <p>Spark est un framework de calcul distribué de données et propose une fonctionnalité
                            de traitement en micro-batch simulant du temps réel : Spark Streaming. Nous allons
                            coder un batch Spark Streaming récupérant les données de notre cluster Kafka.</p>
                    </section>
                    <section>
                        <p>Développons un traitement qui analysera la charge moyenne de CPU des 5
                            dernières secondes.</p>
                    </section>
                </section>

                <section>
                    <h2>Step 5_1</h2>
                    <section></section>
                    <section>
                        <p>Nous allons commencer par créer un context de streaming spark.</p>
                        <p>Vous aurez également besoin d'instancier une configuration Spark.
                            Le "master" de cette configuration doit être "local[2]" afin de
                            faire fonctionner Spark sur votre machine.</p>
                    </section>
                    <section>
                        <h4>Indice</h4>
                    </section>
                    <section>
                        <p>Instancier la classe org.apache.spark.streaming.kafka.StreamingContext</p>
                    </section>
                    <section>
                        <h4>Solution</h4>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def createStreamContext(): StreamingContext = {
    val conf = new SparkConf()
        .setMaster("local[2]")
        .setAppName("kafka-spark-streaming")

    new StreamingContext(new SparkContext(conf), Seconds(5))
}
                        </code></pre>
                    </section>
                </section>

                <section>
                    <h2>Step 5_2</h2>
                    <section></section>
                    <section>
                        <p>Nous allons maintenant créer le stream de données.</p>
                        <p>Regarder du côter de la classe org.apache.spark.streaming.kafka.KafkaUtils</p>
                    </section>
                    <section>
                        <h4>Solution</h4>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
def createStream(context: StreamingContext): InputDStream[(String, String)] = {
    val zookeeper = "localhost:2181"
    val clientName = "streaming-client"
    val topics = Map("handsonkafka" -> 4)
    KafkaUtils.createStream(context, zookeeper, clientName, topics)
}
                        </code></pre>
                    </section>
                </section>

                <section>
                    <h2>Step 5_3</h2>
                    <section></section>
                    <section>
                        <p>La dernière étape consiste à traiter les données que l'on reçoit.
                            Nous allons donc travailler sur le stream de données.</p>
                    </section>
                    <section>
                        <h4>Indices</h4>
                    </section>
                    <section>
                        Le stream contient des tuples clés/valeurs. Nous ne nous intéresserons ici qu'aux valeurs.
                    </section>
                    <section>
                        La fonction "map" d'un stream applique une transformation sur le stream de données.
                    </section>
                    <section>
                        Spark travaille sur un concept de RDD (Resillient Distributed DataSet) qui est une collection
                        d'objets distribuée. La fonction "foreachRDD" permet d'appliquer une fonction qui ait
                        besoin de tous les éléments de la collection.
                    </section>
                    <section>
                        <h4>Solution</h4>
                    </section>
                    <section>
                        <pre><code class="hljs" data-trim>
stream.map(_._2)
    .map(extractValueFromRecord)
    .foreachRDD(rdd => displayAvg(rdd))

streamingContext.start()
streamingContext.awaitTermination()
                        </code></pre>
                    </section>
                </section>


                <section>
                    <h2>Pour résumer</h2>
                    <section></section>
                    <section>
                        <p>Nous avons vu ensemble:<ul>
                            <li>les rôles de Kafka et Zookeeper</li>
                            <li>comment créer des topics et partitions</li>
                            <li>comment produire et consommer de la données</li>
                            <li>la différence entre les deux styles de l'API Kafka</li>
                    </ul></p>
                    </section>
                    <section>
                        <p>Kafka est un journal de log distribué, hautement disponible et performant.</p>
                        <p>Cette solution peut vraiment être considérée comment une base de donneés.</p>
                        <p>Son stockage de données est sûr et la persistence est au coeur même du système.</p>
                    </section>
                    <section>
                        <p>Ces caractéristiques en font aujourd'hui une plateforme incontournable dans l'univers BigData.</p>
                    </section>
                    <section>
                        <p>N'utilisez pas Kafka comme on utilise RabbitMQ, Kafka n'est pas un broker JMS.</p>
                    </section>
                    <section>
                        <p>WATCH OUT</p>
                        <p>Kafka 0.9 va encore plus loin.</p>
                    </section>
                    <section>
                        <p>Jetez un oeil à Confluent.io</p>
                    </section>
                    <section>
                        <p>Et en plus, Kafka est écrit en Scala, et ça c'est cool!</p><p>:-)</p>
                    </section>
                </section>
                
                
                <section>
                    <h1>Merci</h1>
                </section>

                
                
            </div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom


				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
